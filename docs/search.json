[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\nCode\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nHi! Welcome to my website. I’m Morgan, a student at Pomona College and my website showcases some of my favorite things including wildlife, Netflix shows and rugby, sport that I play in college. I’m interested in pursuing a career in healthcare or neuroscience research."
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "Racial disparities in traffic and pedestrian stops in three similar California cities",
    "section": "",
    "text": "Traffic stops are a critical point of interaction between law enforcement and the public, and they provide insight into potential disparities in policing. This project examines traffic stop data from three California cities—San Francisco, Oakland, and San Jose—drawn from the Stanford Open Policing Project. The goal of this analysis is to explore the relationship between city-level enforcement practices and racial disparities in traffic stops. I aim to identify patterns that reveal how race may influence stop frequency and stop type (vehicular vs pedestrian).\nI selected these cities because they are all major urban areas within the same region of California, which allows for meaningful comparisons of traffic stop patterns because they share broad structural similarities (state-level policies, large population size, etc.).\n\n\nCode\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ncon_traffic &lt;- DBI::dbConnect(\n\n  RMariaDB::MariaDB(),\n\n  dbname = \"traffic\",\n\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n\n)\n\n\n\n\nCode\nSELECT \n    city,\n    subject_race AS race,\n    COUNT(*) AS n_stops,\n    SUM(CASE WHEN type = 'vehicular' THEN 1 ELSE 0 END) AS vehicular_stops,\n    SUM(CASE WHEN type = 'pedestrian' THEN 1 ELSE 0 END) AS pedestrian_stops\nFROM (\n    SELECT subject_race, date, type, 'San Francisco' AS city \n      FROM ca_san_francisco_2020_04_01\n    UNION ALL\n    SELECT subject_race, date, type, 'Oakland' AS city \n      FROM ca_oakland_2020_04_01\n    UNION ALL\n    SELECT subject_race, date, type, 'San Jose' AS city \n      FROM ca_san_jose_2020_04_01\n) AS combined\nWHERE subject_race IS NOT NULL\nGROUP BY city, race\nORDER BY city, race;\n\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot(ca_combined_summary, aes(x = race, y = n_stops, fill = city)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Traffic Stops by Race in San Francisco, Oakland, and San Jose\",\n    x = \"Race\",\n    y = \"Number of Stops\",\n    fill = \"City\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThis graph compares the total number of traffic stops for each racial group in San Francisco, Oakland, and San Jose. The pattern differs substantially across cities: San Francisco shows the highest number of stops for white individuals, Oakland shows the highest stop count for Black individuals, and San Jose has the highest stop count for Hispanic individuals.\nThese differences highlight that racial distributions in stop counts are not uniform across the three cities. However, interpreting these patterns requires caution, because each city has a different population size and racial composition. The raw stop counts cannot be interpreted as “stop rates,” and they do not directly indicate disparities on their own. Instead, the variation suggests that each city’s combination of demographics, policing strategies, and enforcement contexts may shape the overall distribution of stops. Additional population-adjusted calculations would be necessary to determine whether these differences reflect disproportionate impact or simply reflect the underlying demographics of each city.\n\n\nCode\n\nSELECT \n    city,\n    race,\n    SUM(n_stops) AS total_stops,\n    SUM(pedestrian_stops) AS total_pedestrian,\n    100.0 * SUM(pedestrian_stops) / SUM(n_stops) AS pedestrian_pct\nFROM (\n    SELECT \n        city,\n        subject_race AS race,\n        COUNT(*) AS n_stops,\n        SUM(CASE WHEN type = 'pedestrian' THEN 1 ELSE 0 END) AS pedestrian_stops\n    FROM (\n        SELECT subject_race, type, 'San Francisco' AS city\n        FROM ca_san_francisco_2020_04_01\n        UNION ALL\n        SELECT subject_race, type, 'Oakland' AS city\n        FROM ca_oakland_2020_04_01\n        UNION ALL\n        SELECT subject_race, type, 'San Jose' AS city\n        FROM ca_san_jose_2020_04_01\n    ) AS combined\n    WHERE subject_race IS NOT NULL\n    GROUP BY city, subject_race\n) AS grouped\nGROUP BY city, race\nORDER BY city, race;\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\nggplot(ca_plot2, aes(x = race, y = pedestrian_pct, fill = city)) +\ngeom_col(position = \"dodge\") +\nlabs(\ntitle = \"Percent of Stops that Are Pedestrian Stops\",\nx = \"Race\",\ny = \"Pedestrian Stop Percentage\"\n) +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\nThis graph shows the percentage of all stops that were pedestrian stops for each racial group across the three cities. The distribution varies by city. In San Jose, Black individuals have a noticeably higher share of pedestrian stops compared with Asian/Pacific Islander individuals, suggesting racial differences in how pedestrian versus vehicular stops occur. In Oakland, the percentages across racial groups are more similar, indicating that pedestrian stops there are not concentrated among any single group. San Francisco shows no pedestrian-stop data in this dataset, which limits cross-city comparisons and highlights differences in reporting or enforcement practices.\nThese observations point to meaningful variation in how cities conduct pedestrian stops, but they do not directly show whether the differences result from demographics, law enforcement strategy, neighborhood patterns, or other factors. To determine whether the differences reflect inequitable policing, additional contextual information—such as pedestrian population estimates or neighborhood-level exposure—would be necessary.\nThe datasets I used for this analysis were drawn from the following source:\nPierson, E., Simoiu, C., Overgoor, J., Corbett-Davies, S., Ramachandran, V., Phillips, C., & Goel, S. (2020). A large-scale analysis of racial disparities in police stops across the United States. Nature Human Behaviour, 4(7), 736–745. Stanford Open Policing Project (2020). Traffic stops database. https://openpolicing.stanford.edu/"
  },
  {
    "objectID": "whales and dolphins.html",
    "href": "whales and dolphins.html",
    "title": "Cetacean species",
    "section": "",
    "text": "This dataset compiles all Cetacean species and their birth information: their birth year, sex, location, mother, father, status, etc. This plot shows the total births per year by species.\n\n\n\n\n\n\n\n\n\nThe Cetacean dataset I worked with comes from the TidyTuesday database.\nThe original data set was made by Amber Thomas in her article for The Pudding.\n[link] TidyTuesday (2018-12-18)\nThomas, Amber, Cetacean Insights: What We’ve Learned From Whales and Dolphins in Captivity. The Pudding 2017. https://pudding.cool/2017/07/cetaceans/"
  },
  {
    "objectID": "seasons.html",
    "href": "seasons.html",
    "title": "American Idol Seasons",
    "section": "",
    "text": "This dataset compiles the American idol winners and runner ups of the first 18 seasons, as well as release date, judges’ names, finals venue, number of episodes of the season, and original network and host. This plot compares judges’ appearance across all seasons.\n\n\n\n\n\n\n\n\n\nThe American Idol dataset I worked with comes from the TidyTuesday database.\n[link] https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-07-23/readme.md\nThe original dataset comes from Wikipedia.\nhttps://en.wikipedia.org/wiki/American_Idol"
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Ethics: The Big Data era of Mosaicked Deidentification",
    "section": "",
    "text": "In 2006, Netflix, a large streaming service, released a dataset containing nearly 500,000 users’ movie ratings as part of a public machine-learning competition called the Netflix Prize. Netflix claimed that all personal information had been removed to protect user privacy (Leetaru, 2016). The purpose of releasing the dataset was to allow researchers to develop improved recommendation algorithms—that is, models that predict what movies a user might enjoy based on their past ratings. Although names and account IDs were removed, the dataset preserved detailed behavioral traces such as timestamps and rating patterns, which can be uniquely identifying. Additionally, multiple datasets can be combined to fill in one another’s missing pieces, a process known as “mosaicking,” which illustrates why seemingly anonymized data can still reveal identities (Leetaru, 2016).\nIn 2008, researchers Arvind Narayanan and Vitaly Shmatikov demonstrated that the Netflix Prize dataset was vulnerable to deanonymization by matching users’ Netflix rating patterns with publicly available rating histories from IMDb (Narayanan & Shmatikov, 2008). Even if someone rated only a handful of movies on IMDb, the combination of rating values and timestamps was often specific enough to identify their full Netflix record with high accuracy. This showed that the dataset—though intended for research—posed broader privacy risks: individuals’ preferences related to politics, sexuality, religion, or personal interests could be exposed without their knowledge or consent.\nWas the consent structure and permission followed?\nAlthough Netflix removed explicit identifiers, users were never informed that their ratings would be shared publicly for a research competition. No explicit permission was requested, and the release relied on the assumption that anonymization alone made the data safe (Leetaru, 2016). The users did not consent to this secondary use of their data. Moreover, because the dataset could be deanonymized, Netflix’s anonymization was insufficient to provide meaningful privacy protection. Individuals had no reasonable expectation that their private rating histories could be reconstructed and linked back to them using external data sources. The ethical issue is that Netflix treated anonymization as a substitute for consent, despite expert evidence that behavioral data is highly identifiable (Narayanan & Shmatikov, 2008).\nWas the data being used in unintended ways?\nYes. Netflix released the dataset specifically to improve recommendation algorithms, but the data was not intended to be cross-referenced with external platforms to re-identify users. Narayanan and Shmatikov (2008) showed that such cross-linking was not only possible but highly accurate. This is an example of secondary use, where data collected for one purpose (recommender systems) is repurposed for another (deanonymization research), highlighting a key risk in data science. Users’ rating histories—originally meant only for their own viewing recommendations—became a potential vehicle for revealing sensitive personal information.\nWas the data actually anonymized?\nThe data was only partially anonymized. Netflix removed names and replaced account IDs, but retained users’ complete behavioral profiles, which function like digital fingerprints. Rating sequences, timing patterns, and movie choices are distinctive enough to identify individuals with surprising accuracy. Narayanan and Shmatikov (2008) demonstrated that knowing a user’s approximate ratings on just a few films on IMDb could be sufficient to re-identify that user in the Netflix dataset. Once linked, the person’s full Netflix viewing history (content related to sexuality, mental health, religion, or politics) became exposed. This case illustrates that anonymization is not equivalent to privacy.\nWho was harmed, and who benefitted?\nNetflix benefited from crowdsourcing improvements to its recommendation algorithms at no financial cost, and academic researchers gained publications and benchmark data from the competition. However, the individuals harmed were ordinary Netflix users whose rating histories were shared without explicit permission. Some users who were deanonymized had sensitive movie preferences that could reveal personal identity attributes. Even without malicious intent, releasing re-identifiable behavioral data created real privacy risks once anonymization proved ineffective.\nWho was measured, and can results be generalized?\nThe dataset represents only users who actively rate movies on Netflix, a non-random and self-selected subgroup. These users may differ from the general Netflix population in age, engagement, genre preferences, and rating behavior. Therefore, models trained on the Netflix Prize dataset generalize poorly to users who rarely rate movies or who consume content without providing feedback. This introduces bias and limits the external validity of any models or conclusions derived from the dataset.\nWhy does this example matter for data science?\nThe Netflix Prize case demonstrates how easy it is to re-identify individuals from behavioral data that companies claim is anonymous. It challenges the assumption that removing names is sufficient for privacy protection and highlights the fact that human behavior itself is highly identifiable. It also exposes a power imbalance: companies control the data and may share it widely for their own benefit, while users are often unaware their information is being repurposed in ways that could expose them to harm. The incident remains a foundational example in data ethics, showing the need for stronger protections, transparent consent processes, and caution when releasing real-world datasets.\nReferences\nLeetaru, K. (2016). The Big Data Era of Mosaicked Deidentification: Can We Anonymize Data Anymore? Forbes.\nNarayanan, A., & Shmatikov, V. (2008). Robust de-anonymization of large sparse datasets. IEEE Symposium on Security and Privacy, 111–125."
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Netflix text analysis",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\n\nnetflix_titles &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-20/netflix_titles.csv')\n\nnetflix &lt;- netflix_titles\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(scales)\n\n\nnetflix_love_war &lt;- netflix |&gt;\n  filter(str_detect(title, \"(?&lt;!\\\\w)(Love|War)(?!\\\\w)\")) |&gt;\n  select(title, type, country, release_year)\n\ntotal_by_year &lt;- netflix |&gt;\n  count(release_year, name = \"total_titles\")\n\nlove_war_by_year &lt;- netflix_love_war |&gt;\n  count(release_year, name = \"love_war_titles\")\n\nlove_war_prop &lt;- left_join(love_war_by_year, total_by_year, by = \"release_year\") |&gt;\n  mutate(prop_titles = love_war_titles / total_titles)\n\n\nggplot(love_war_prop, aes(x = release_year, y = prop_titles)) +\ngeom_col(fill = \"pink\") +\ngeom_line(color = \"red\", size = 1) +\nlabs(\ntitle = \"Proportion of Netflix Titles Containing 'Love' or 'War' Over Time\",\nx = \"Release Year\",\ny = \"Proportion of Titles\"\n) +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot shows the proportion of titles with the words “Love” and “War” over time. It reflects that the shows released were highly correlated with historical context. Indeed, we see that the highest peak was during the Second World War.\n\n\nCode\nnetflix_trends &lt;- netflix_love_war |&gt;\nmutate(\ntype = factor(type),\ncontains_heart_battle = TRUE, \nfirst_word = word(title, 1)\n) |&gt;\nfilter(!is.na(release_year))\n\nnetflix_proportions &lt;- netflix_trends |&gt;\ngroup_by(release_year, type) |&gt;\nsummarise(n_titles = n(), .groups = \"drop\") |&gt;\ngroup_by(release_year) |&gt;\nmutate(prop_titles = n_titles / sum(n_titles)) |&gt;\nungroup()\n\n\nggplot(netflix_proportions, aes(x = release_year, y = prop_titles, fill = type)) +\ngeom_col(position = \"stack\") +\nscale_fill_manual(values = c(\"Movie\" = \"steelblue\", \"TV Show\" = \"orange\")) +\nscale_y_continuous(labels = percent_format(accuracy = 1)) +\nlabs(\nx = \"Release Year\",\ny = \"Proportion of Titles\",\nfill = \"Content Type\",\ntitle = \"Proportion of Movies vs TV Shows on Netflix Over Time\"\n) +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot shows the proportion of movies and TV shows available on Netflix over time. It is interesting to see an increase in TV shows in more recent years.\n\n\nCode\nnetflix_genre &lt;- netflix_titles |&gt;\nmutate(\nmain_genre = str_extract(listed_in, \"^[^,]+\"),\nmain_genre_upper = str_to_upper(main_genre),\nis_action = str_detect(main_genre, \"Action\")\n) |&gt;\ndrop_na(release_year, main_genre_upper) |&gt;\nfilter(release_year &gt;= 1950)\n\n\n\ntop_genres &lt;- netflix_genre |&gt;\ncount(main_genre_upper, sort = TRUE) |&gt;\nslice_head(n = 6) |&gt;\npull(main_genre_upper)\n\n\n\nnetflix_top_genres &lt;- netflix_genre |&gt;\nfilter(main_genre_upper %in% top_genres) |&gt;\nmutate(\nperiod = case_when(\nrelease_year &lt; 2010 ~ \"Before 2010\",\nrelease_year &gt;= 2010 & release_year &lt; 2020 ~ \"2010–2019\",\nrelease_year &gt;= 2020 ~ \"2020–Present\"\n),\nperiod = factor(period, levels = c(\"Before 2010\", \"2010–2019\", \"2020–Present\"))\n)\n\n\ngenre_trends &lt;- netflix_top_genres |&gt;\ncount(period, main_genre_upper, is_action) |&gt;\ngroup_by(period) |&gt;\nmutate(prop = n / sum(n)) |&gt;\nungroup()\n\nggplot(genre_trends, aes(x = period, y = prop, fill = main_genre_upper)) +\ngeom_col(position = position_dodge(width = 0.8)) +\nscale_y_continuous(labels = percent_format(accuracy = 1)) +\nlabs(\ntitle = \"Evolution of Top Netflix Genres Over Time\",\nsubtitle = \"Each genre shown separately, one color per genre\",\nx = \"Release Period\",\ny = \"Proportion of Titles\",\nfill = \"Genre\"\n) +\ntheme_minimal(base_size = 9) +\ntheme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nThis last plot evolution in proportion of Netflix titles per genre. Following the same theme as previously, we can see it follows the audience demand, and historical context. For example, we can see an increase from 5% to 25% of titles that are international TV shows, which emphasizes a broadening target audience.\nOver time, Netflix has shifted to an increasing number of TV shows comparing to movies. This is to address increase in demand, and cultural shift towards TV shows. We also notice less of an emphasis on movies with variations of Love and War, highly associated with a specific time period (mainly, the Second World War), which makes sense when looking at the historical context. In addition, there has been a shift in genre, to reach more people. There are more international TV shows released in recent years, and fewer action movies. This shows that Netflix shows are shifting in multiple ways to address increasing public demand, and societal changes.\nOne thing to be mindful of, is that the number of samples is limited from the 1940s to the 1990s, which explains why the values we see are so extreme, but don’t exactly reflect reality.\nShivan Bansal scraped this data and made it publicly available on Kaggle at the following link:\nhttps://www.kaggle.com/datasets/shivamb/netflix-shows\nThe dataset I used lives in the Tidy Tuesday database."
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Diabetes simulation study",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nglobal_mortality &lt;- read_csv(\"global_mortality.csv\")\nview(global_mortality)\n\n\nThis dataset presents the mortality rate for different diseases, per year (from 1990 to 2016) and per country. We are exploring the specific average rate of mortality from diabetes in the United States and in France across all years. The dataset I analyzed comes from the Tidy Tuesday database and is available at the following link. https://github.com/rfordatascience/tidytuesday/tree/main/data/2018/2018-04-16\nThe original data comes from an article entitled, “Causes of death” and published in the Our World in Data magazine by Saloni Dattani, Fiona Spooner, Hannah Ritchie and Max Roser at the following link: Dattani, S., Spooner, F., Ritchie, H., & Roser, M. (2023). Causes of Death. Our World in Data. https://ourworldindata.org/causes-of-death\nIn this simulation study, we will explore whether France and the United States have the same average diabetes mortality rate. We will first calculate the observed difference in mean diabetes mortality between the two countries across all available years (1990-2016). To test whether the difference we observe could have been a result of random distribution, we will run a permutation test shuffling the diabetes mortality percentage rate per country 1000 times, to create a null distribution. By treating the observed years as a sample, we can use a permutation test to assess whether the observed difference in average mortality is larger than we would expect from random variation in yearly mortality. This allows us to generalize conclusions beyond the years in the dataset.\nThe research question we are answering is the following: Do the US and France differ in underlying diabetes mortality rates, beyond what might occur by chance?\nThe null hypothesis (Ho): The underlying process generating the diabetes mortality rate is the same in France and the US. Any observed difference in yearly mortality rates is due to random variation across years.\nThe alternative hypothesis (Ha): The underlying process differs between the two countries, leading to different average diabetes mortality rates.\n\n\nCode\nlibrary(dplyr)\n\nmortality_filtered &lt;- global_mortality |&gt;\n  filter(country %in% c(\"France\", \"United States\")) |&gt;\n  select(country, year, `Diabetes (%)`)\n\n\nmortality_filtered &lt;- mortality_filtered |&gt;\n  rename(diabetes_pct = `Diabetes (%)`)\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\nlibrary(dplyr)\n\nmortality_avg &lt;- mortality_filtered |&gt;\n  group_by(country) |&gt;\n  summarize(avg_diabetes = mean(`diabetes_pct`))\n\nmortality_avg\n\n\n# A tibble: 2 × 2\n  country       avg_diabetes\n  &lt;chr&gt;                &lt;dbl&gt;\n1 France                4.72\n2 United States         6.49\n\n\nCode\nggplot(mortality_avg, aes(x = country, y = avg_diabetes, fill = country)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Average Diabetes Mortality (%) Across All Years\",\n       x = \"Country\",\n       y = \"Average Diabetes Mortality (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe main variable of interest is the percentage of diabetes mortality rate. While this dataset does not provide a detailed description, we infer that it represents the percentage of deaths in a given year that were attributed to diabetes.\nWe observe a difference in average mortality rate between the two countries, so now we are going to test if this is a result of the random distribution model, by establishing a null distribution. Instead of comparing only the two countries’ averages, we will repeatedly shuffle the yearly diabetes mortality percentages between the two countries, recompute the average mortality for each country, and record the difference. By repeating this process 1000 times, we can see how large the difference would be between the two countries if we were following the random chance model.\n\n\nCode\nlibrary(purrr)\n\nobs_diff &lt;- diff(mortality_avg$avg_diabetes)\nobs_diff\n\n\n[1] 1.773729\n\n\nCode\nperm_data &lt;- function(rep, data) {\n  data |&gt;\n    select(country, `diabetes_pct`) |&gt;\n    mutate(diabetes_perm = sample(`diabetes_pct`, replace = FALSE)) |&gt;\n    group_by(country) |&gt;\n    summarize(\n      perm_ave = mean(diabetes_perm)\n    ) |&gt;\n    summarize(\n      perm_diff = diff(perm_ave),\n      rep = rep\n    )\n}\n\nset.seed(47)\nperm_stats &lt;- map(1:1000, perm_data, data = mortality_filtered) |&gt;\n  list_rbind()\n\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot(perm_stats, aes(x = perm_diff)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"pink\", color = \"black\", alpha = 0.5) +\n  geom_vline(aes(xintercept = obs_diff), color = \"red\", linetype = \"dashed\", size = 1.2) +\n  labs(title = \"Null Distribution of Difference in Diabetes Mortality (%)\",\n       subtitle = \"France vs United States (All Years)\",\n       x = \"Difference in Mean Mortality (France - U.S.)\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOur p-value is 0, which is less than 0.05 so this is strong evidence to reject our null hypothesis.\nWe can reject the hypothesis that the process by which the diabetes mortality rate between France and the United States is generated across all years (1990-2016) is different for each country.\nThe permutation test gives a very small p-value, suggesting that the difference in diabetes mortality between the US and France is unlikely to be due to random variation alone."
  },
  {
    "objectID": "my_slides.html#background",
    "href": "my_slides.html#background",
    "title": "Ethics on Netflix",
    "section": "Background",
    "text": "Background\n\nNetflix launched a public competition in 2006, goal is to predict how a user would rate a movie they hadn’t seen (split by team, $1million in prize money if you can improve system by 10%).\nGoal: improve movie recommendation algorithms that work using predictions based on what a user has previously watched\nPart of Netflix’s broader push to personalize user experience, increasing use of the platform, and user satisfaction"
  },
  {
    "objectID": "my_slides.html#methods",
    "href": "my_slides.html#methods",
    "title": "Ethics on Netflix",
    "section": "Methods",
    "text": "Methods\n\n500,000 users\nAbout 17,770 movies rated on a scale of 1-5 (1 being “dislike”, 5 being “love) with associated timestamp of rating, movie ID, user ID and account information\n=&gt; user ID and account information were removed for release for this competition"
  },
  {
    "objectID": "my_slides.html#terms-of-service-and-consent",
    "href": "my_slides.html#terms-of-service-and-consent",
    "title": "Ethics on Netflix",
    "section": "Terms of service and consent",
    "text": "Terms of service and consent\n\nNetflix never notified users about the data release for this competition\nNo explicit permission\nAssumed anonymization = consent (assuming it is safe because names were removed)\nTerms of service at the time allowed data use for “improving services,” ensured it wasn’t for public release"
  },
  {
    "objectID": "my_slides.html#deanonymization",
    "href": "my_slides.html#deanonymization",
    "title": "Ethics on Netflix",
    "section": "Deanonymization",
    "text": "Deanonymization\n\nNarayanan & Shmatikov (computer scientists, with expertise in data privacy) re-identified users in 2008 by matching Netflix ratings with public IMDb ratings\nOnly needed a few movies + approximate timestamps to identify people (each set of movies watched by an individual is unique)\nHigh accuracy: Netflix records could be linked to specific individuals\nRevealed personal preferences (politics, sexuality, religion, etc.) through viewing history"
  },
  {
    "objectID": "my_slides.html#hidden-identifiers",
    "href": "my_slides.html#hidden-identifiers",
    "title": "Ethics on Netflix",
    "section": "Hidden identifiers",
    "text": "Hidden identifiers\n\nNames removed, but unique behavioral fingerprints remained\nTimestamps, movies rated and rating styles (what a “good” movie’s rating is) are unique\nMosaicking: combining datasets from different sources to re-identify users"
  },
  {
    "objectID": "my_slides.html#bias",
    "href": "my_slides.html#bias",
    "title": "Ethics on Netflix",
    "section": "Bias",
    "text": "Bias\n\nDataset is biased toward:\n\nheavy users\nhighly engaged users\nspecific demographics (more men, younger population, highly educated groups)"
  },
  {
    "objectID": "my_slides.html#why-does-this-matter",
    "href": "my_slides.html#why-does-this-matter",
    "title": "Ethics on Netflix",
    "section": "Why does this matter?",
    "text": "Why does this matter?\n\nShows how “anonymous” data can still identify people\nProves removing names ≠ privacy\nDemonstrates potential risks of secondary use (use for other purposes)\nExposes power imbalance: companies control data, users don’t\nIt is a good case study for privacy risks =&gt; companies implement stricter consent policies"
  },
  {
    "objectID": "my_slides.html#why-was-i-interested-in-this",
    "href": "my_slides.html#why-was-i-interested-in-this",
    "title": "Ethics on Netflix",
    "section": "Why was I interested in this?",
    "text": "Why was I interested in this?\n\nExpands on stakes of another class I’m taking covering themes of ethics, surveillance and privacy\nNetflix User"
  }
]