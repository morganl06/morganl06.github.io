[
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Diabetes simulation study",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nglobal_mortality &lt;- read_csv(\"global_mortality.csv\")\nview(global_mortality)\n\n\nThis dataset presents the mortality rate for different diseases, per year (from 1990 to 2016) and per country. We are exploring the specific average rate of mortality from diabetes in the United States and in France across all years. https://github.com/rfordatascience/tidytuesday/tree/main/data/2018/2018-04-16\nThe original data comes from an article entitled, Causes of death and published in the Our World in Data magazine by Saloni Dattani, Fiona Spooner, Hannah Ritchie and Max Roser at the following link:\nDattani, S., Spooner, F., Ritchie, H., & Roser, M. (2023). Causes of Death. Our World in Data. https://ourworldindata.org/causes-of-death\nIn this simulation study, we will explore whether France and the United States have the same average diabetes mortality rate. We will first calculate the observed difference in mean diabetes mortality between the two countries across all available years (1990-2016). To test whether the difference we observe could have been a result of random distribution, we will run a permutation test shuffling the diabetes mortality percentage rates countries 1000 times, to create a null distribution.\nMy research question is the following: do the United States and France have different mortality rates for diabetes on average from 1990 to 2006?\nThe null hypothesis (Ho): On average, there is no difference in mortality rates for diabetes.\nThe alternative hypothesis (Ha): On average, there is a difference in mortality rates for diabetes.\n\n\nCode\nlibrary(dplyr)\n\nmortality_filtered &lt;- global_mortality |&gt;\n  filter(country %in% c(\"France\", \"United States\")) |&gt;\n  select(country, year, `Diabetes (%)`)\n\n\nmortality_filtered &lt;- mortality_filtered |&gt;\n  rename(diabetes_pct = `Diabetes (%)`)\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\nlibrary(dplyr)\n\nmortality_avg &lt;- mortality_filtered |&gt;\n  group_by(country) |&gt;\n  summarize(avg_diabetes = mean(`diabetes_pct`))\n\nmortality_avg\n\n\n# A tibble: 2 × 2\n  country       avg_diabetes\n  &lt;chr&gt;                &lt;dbl&gt;\n1 France                4.72\n2 United States         6.49\n\n\nCode\nggplot(mortality_avg, aes(x = country, y = avg_diabetes, fill = country)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Average Diabetes Mortality (%) Across All Years\",\n       x = \"Country\",\n       y = \"Average Diabetes Mortality (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe observe a difference in average mortality rate between the two countries, so now we are going to test if this is a result of the random distribution model, by establishing a null distribution. Instead of comparing only the two countries’ averages, we will repeatedly shuffle the yearly diabetes mortality percentages between the two countries, recompute the average mortality for each country, and record the difference. By repeating this process 1000 times, we can see how large the difference would be between the two countries if we were following the random chance model.\n\n\nCode\nlibrary(purrr)\n\nobs_diff &lt;- diff(mortality_avg$avg_diabetes)\nobs_diff\n\n\n[1] 1.773729\n\n\nCode\nperm_data &lt;- function(rep, data) {\n  data |&gt;\n    select(country, `diabetes_pct`) |&gt;\n    mutate(diabetes_perm = sample(`diabetes_pct`, replace = FALSE)) |&gt;\n    group_by(country) |&gt;\n    summarize(\n      perm_ave = mean(diabetes_perm)\n    ) |&gt;\n    summarize(\n      perm_diff = diff(perm_ave),\n      rep = rep\n    )\n}\n\nset.seed(47)\nperm_stats &lt;- map(1:1000, perm_data, data = mortality_filtered) |&gt;\n  list_rbind()\n\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot(perm_stats, aes(x = perm_diff)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"pink\", color = \"black\", alpha = 0.5) +\n  geom_vline(aes(xintercept = obs_diff), color = \"red\", linetype = \"dashed\", size = 1.2) +\n  labs(title = \"Null Distribution of Difference in Diabetes Mortality (%)\",\n       subtitle = \"France vs United States (All Years)\",\n       x = \"Difference in Mean Mortality (France - U.S.)\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\np-value = 0 (&lt; 0.05) so this is strong evidence to reject our null hypothesis.\nWe can reject the hypothesis that the mortality rates are the same for diabetes across all years (1990-2016).\nThere is strong evidence that the alternative hypothesis stating that France and the US have different average mortality rates for diabetes from 1990-2016 is true."
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Netflix text analysis",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\n\nnetflix_titles &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-20/netflix_titles.csv')\n\nnetflix &lt;- netflix_titles\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\n\nnetflix_love_war &lt;- netflix |&gt;\n  filter(\n    str_detect(title, regex(\"(?&lt;!\\\\w)Love|War(?!\\\\w)\", ignore_case = TRUE)) \n  ) |&gt;\n  select(title, type, country, release_year)\n\ntotal_by_year &lt;- netflix |&gt;\n  count(release_year, name = \"total_titles\")\n\nlove_war_by_year &lt;- netflix_love_war |&gt;\n  count(release_year, name = \"love_war_titles\")\n\nlove_war_prop &lt;- left_join(love_war_by_year, total_by_year, by = \"release_year\") |&gt;\n  mutate(prop_titles = love_war_titles / total_titles)\n\nggplot(love_war_prop, aes(x = release_year, y = prop_titles)) +\n  geom_col(fill = \"pink\") +\n  geom_line(color = \"red\", size = 1) +\n  labs(\n    title = \"Proportion of Netflix Titles Containing 'Love' or 'War' Over Time\",\n    x = \"Release Year\",\n    y = \"Proportion of Titles\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot shows the proportion of titles with the words “Love” and “War” over time. It reflects that the shows released were highly correlated with historical context. Indeed, we see that the highest peak was during the Second World War.\n\n\nCode\nnetflix_trends &lt;- netflix_love_war |&gt;\n  mutate(\n    type = factor(type),\n    contains_heart_battle = str_detect(title, regex(\"Love|War\", ignore_case = TRUE)), \n    first_word = str_extract(title, \"^\\\\S+\"),       \n  ) |&gt;\n  filter(!is.na(release_year))\n\nnetflix_proportions &lt;- netflix_trends |&gt;\n  group_by(release_year, type) |&gt;\n  summarise(n_titles = n(), .groups = \"drop\") |&gt;\n  group_by(release_year) |&gt;\n  mutate(prop_titles = n_titles / sum(n_titles)) |&gt;\n  ungroup()\n\nggplot(netflix_proportions, aes(x = release_year, y = prop_titles, fill = type)) +\n  geom_col(position = \"stack\") +\n  scale_fill_manual(values = c(\"Movie\" = \"steelblue\", \"TV Show\" = \"orange\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(\n    x = \"Release Year\",\n    y = \"Proportion of Titles\",\n    fill = \"Content Type\",\n    title = \"Proportion Movies vs TV shows available on Netflix over time\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot shows the proportion of movies and TV shows available on Netflix over time. It is interesting to see an increase in TV shows in more recent years.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\n\n\nnetflix_genre &lt;- netflix_titles |&gt;\n  mutate(\n    main_genre = str_extract(listed_in, \"^[^,]+\"),  \n    main_genre_upper = str_to_upper(main_genre),    \n    is_action = str_detect(main_genre, \"Action\")   \n  ) |&gt;\n  drop_na(release_year, main_genre_upper) |&gt;\n  filter(release_year &gt;= 1950)\n\ntop_genres &lt;- netflix_genre |&gt;\n  count(main_genre_upper, sort = TRUE) |&gt;\n  slice_head(n = 6) |&gt;\n  pull(main_genre_upper)\n\nnetflix_top_genres &lt;- netflix_genre |&gt;\n  filter(main_genre_upper %in% top_genres) |&gt;\n  mutate(\n    period = case_when(\n      release_year &lt; 2010 ~ \"Before 2010\",\n      release_year &gt;= 2010 & release_year &lt; 2020 ~ \"2010–2019\",\n      release_year &gt;= 2020 ~ \"2020–Present\"\n    ),\n    period = factor(period, levels = c(\"Before 2010\", \"2010–2019\", \"2020–Present\"))\n  )\n\ngenre_trends &lt;- netflix_top_genres |&gt;\n  count(period, main_genre_upper, is_action) |&gt;\n  group_by(period) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  ungroup()\n\nggplot(genre_trends, aes(x = period, y = prop, fill = main_genre_upper)) +\n  geom_col(position = position_dodge(width = 0.8)) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(\n    title = \"Evolution of Top Netflix Genres Over Time\",\n    subtitle= \"Each genre shown separately, one color per genre\",\n    x = \"Release Period\",\n    y = \"Proportion of Titles\",\n    fill = \"Genre\"\n  ) +\n  theme_minimal(base_size = 9) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nThis last plot evolution in proportion of Netflix titles per genre. Following the same theme as previously, we can see it follows the audience demand, and historical context. For example, we can see an increase from 5% to 25% of titles that are international TV shows, which emphasizes a broadening target audience.\nOver time, Netflix has shifted to an increasing number of TV shows comparing to movies. This is to address increase in demand, and cultural shift towards TV shows. We also notice less of an emphasis on movies with variations of Love and War, highly associated with a specific time period (mainly, the Second World War), which makes sense when looking at the historical context. In addition, there has been a shift in genre, to reach more people. There are more international TV shows released in recent years, and fewer action movies. This shows that Netflix shows are shifting in multiple ways to address increasing public demand, and societal changes.\nOne thing to be mindful of, is that the number of samples is limited from the 1940s to the 1990s, which explains why the values we see are so extreme, but don’t exactly reflect reality.\nThe dataset is collected from Flixable which is a third-party Netflix search engine.\n[link] https://github.com/rfordatascience/tidytuesday/blob/main/data/2021/2021-04-20/readme.md"
  },
  {
    "objectID": "seasons.html",
    "href": "seasons.html",
    "title": "American Idol Seasons",
    "section": "",
    "text": "This dataset compiles the American idol winners and runner ups of the first 18 seasons, as well as release date, judges’ names, finals venue, number of episodes of the season, and original network and host. This plot compares judges’ appearance across all seasons.\n\n\n\n\n\n\n\n\n\nThe American Idol dataset I worked with comes from the TidyTuesday database.\n[link] https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-07-23/readme.md\nThe original dataset comes from Wikipedia."
  },
  {
    "objectID": "whales and dolphins.html",
    "href": "whales and dolphins.html",
    "title": "Cetacean species",
    "section": "",
    "text": "This dataset compiles all Cetacean species and their birth information: their birth year, sex, location, mother, father, status, etc. This plot shows the total births per year by species.\n\n\n\n\n\n\n\n\n\nThe Cetacean dataset I worked with comes from the TidyTuesday database.\nThe original data set was made by Amber Thomas in her article for The Pudding.\n[link] TidyTuesday (2018-12-18)\nThomas, Amber, Cetacean Insights: What We’ve Learned From Whales and Dolphins in Captivity. The Pudding 2017. https://pudding.cool/2017/07/cetaceans/"
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Ethics: The Big Data era of Mosaicked Deidentification",
    "section": "",
    "text": "In 2006, Netflix, a streaming service company, released a dataset of nearly 500,000 users’ movie ratings for a public machine-learning competition, the Netflix Prize, claiming that all personal information had been removed to protect privacy (Leetaru, 2016). The goal of releasing this dataset was to allow researchers to develop improved recommendation algorithms, which predict what movies a user might like based on past ratings. Although Netflix removed names and account IDs, the data contained unique patterns of user behavior that could potentially reveal identities. Moreover, multiple datasets can be combined to fill in each other’s missing information, a concept known as “mosaicking,” which illustrates how supposedly anonymized data can still be re-identified (Leetaru, 2016).\nHowever, in 2008, researchers Arvind Narayanan and Vitaly Shmatikov demonstrated that users could be re-identified by cross-referencing their “anonymous” rating patterns with publicly available profiles on the Internet Movie Database (IMDb). Even if someone had rated only a few movies on IMDb, their Netflix record could be linked with high accuracy. While the dataset was intended for data science research, there was an ethical dilemma centered on privacy: Netflix promised the data was anonymized, but individuals could still be identified, and sensitive information such as political preferences, sexual orientation, religious interests, and personal tastes could be exposed without their knowledge or meaningful consent.\nWas the consent structure and permission followed?\nAlthough Netflix technically removed names and replaced user IDs with random numbers, users were never informed that their data would be shared for research or public competition. The consent structure assumed “anonymization” was enough to make the data safe, so no explicit permission was requested. The users didn’t really consent to their data being used. In reality, the ability to deanonymize users shows that meaningful consent was not possible. People had no way of anticipating that their personal movie choices could be reconstructed and linked back to their identities. The ethical issue is that Netflix believed anonymity eliminated risk, but privacy experts consider streaming data highly identifiable because everyone has a unique pattern of media consumption.\nWas the data being used in unintended ways?\nAlthough the Netflix dataset was released to improve recommendation algorithms, it was not originally intended to be cross-referenced with other platforms to deanonymize users. Narayanan and Shmatikov’s work revealed that the same data could be exploited in ways the participants and Netflix had not anticipated, illustrating a case of “secondary use” risks in data science. Users’ personal preferences, originally intended only for improving movie recommendations, became a source of potential privacy violations when combined with IMDb ratings. This highlights a critical ethical issue: even when data are collected and shared legally, unforeseen uses can expose individuals to harm.\nWas the data actually anonymized?\nThe dataset was only partially anonymized. Names and account IDs were removed, but the behavioral identity of each user (movie choices, timestamps, rating patterns, etc.) was kept intact. Those patterns act like a digital signature: knowing a person’s movie history from another platform is enough to reveal them in the Netflix dataset. Researchers showed that if you know a person rated just 6 movies on IMDb, even approximately, you can identify their entire Netflix record (Narayanan, Shmatikov, 2008). Once that link is possible, a user’s full viewing history (movies about sexuality, mental health, or political interests) can be uncovered. This shows that “anonymous” data can still be personally identifiable, and that anonymization is not the same as privacy.\nWho was harmed, and who benefitted?\nNetflix benefitted by crowdsourcing algorithm improvements without paying the researchers who created models for them. Academic teams also benefited because the dataset helped produce publications and benchmarks. The people harmed were everyday Netflix users who didn’t agree to have their private viewing habits shared publicly. Some users in the deanonymized dataset could be outed for sensitive media preferences such as LGBTQ films, religious documentaries, or political content that could expose personal details about identity or beliefs. Even if Netflix never intended to harm anyone, there was a privacy violation: once de-identification failed, user data became “re-identifiable,” and therefore no longer private.\nWho was measured, and can results be generalized?\nThe Netflix dataset represents users who actively rated movies on the platform, which may not reflect the broader population of movie viewers. These individuals are self-selected and may differ in demographics or viewing habits from the general public. Algorithms trained on this dataset could thus overfit to the behavior of this subgroup, raising questions about fairness, bias, and generalizability of recommendations built from this data.\nWhy does this example matter for data science?\nThis case matters because it shows how easy it is to de-anonymize data that companies claim is safe. It challenges the idea that privacy can be protected simply by removing names. The Netflix example proves that behavior itself (what you watch, when you watch it, what you rate) is a form of identity that we may not think of at first glance. It also highlights a power imbalance between the company and the users: Netflix had control of the data, used it to improve profit-generating algorithms, and exposed users to risk without their knowledge.\nReferences:\nLeetaru, K. (2016), The Big Data Era of Mosaicked Deidentification: Can We Anonymize Data Anymore?, Forbes.\nNarayanan, A., & Shmatikov, V. (2008). Robust de-anonymization of large sparse datasets. In Proceedings – 2008 IEEE Symposium on Security and Privacy (SP) (pp. 111–125). https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4531148"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Morgan L",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nHi! This is my project for DS002! I’m interested in minoring in Data Science! Outside of academics, I play rugby and like to bake with friends."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\nCode\n1 + 1\n\n\n[1] 2"
  }
]