[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\nCode\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hi! Welcome to my website. I’m Morgan, a sophomore at Pomona College. I’m a neuroscience major interested in pursuing a career in healthcare or neuroscience research.\nMy website showcases some of my projects and favorite things including wildlife, Netflix shows, and rugby, sport that I play in college.\nThis is a Quarto website. To learn more about Quarto websites, visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "Racial disparities in traffic and pedestrian stops in Oakland and San Jose",
    "section": "",
    "text": "Traffic stops are a critical point of interaction between law enforcement and the public, and they provide insight into potential disparities in policing. This project examines traffic stop data from two California cities, Oakland, and San Jose, drawn from the Stanford Open Policing Project. The goal of this analysis is to explore the relationship between city-level enforcement practices and racial disparities in traffic stops.\nI selected these cities because they are all major urban areas within the same region of California that I wanted to explore.\n\n\nCode\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ncon_traffic &lt;- DBI::dbConnect(\n\n  RMariaDB::MariaDB(),\n\n  dbname = \"traffic\",\n\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n\n)\n\n\n\n\nCode\n\n# Count total, vehicular, and pedestrian stops by race in Oakland and San Jose\n# First, select race, type, and assign city labels for each dataset\n# Then combine both cities into a single dataset\n\nSELECT \n    city,\n    subject_race AS race,\n    COUNT(*) AS n_stops,\n    SUM(CASE WHEN type = 'vehicular' THEN 1 ELSE 0 END) AS vehicular_stops,\n    SUM(CASE WHEN type = 'pedestrian' THEN 1 ELSE 0 END) AS pedestrian_stops\nFROM (\n    # Oakland data with city label\n    SELECT subject_race, type, 'Oakland' AS city \n    FROM ca_oakland_2020_04_01\n\n    UNION ALL\n\n    # San Jose data with city label\n    SELECT subject_race, type, 'San Jose' AS city \n    FROM ca_san_jose_2020_04_01\n) AS combined\n# Only include rows where race is not missing\nWHERE subject_race IS NOT NULL\n# Group by city and race to get totals\nGROUP BY city, subject_race\n# Sort results by city and race\nORDER BY city, subject_race;\n\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot(ca_total_stops, aes(x = race, y = n_stops, fill = city)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Traffic Stops by Race in Oakland and San Jose\",\n    x = \"Race\",\n    y = \"Number of Stops\",\n    fill = \"City\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThis bar chart shows raw stop counts by race and city. Oakland shows higher stop counts for Black individuals, while San Jose shows higher stop counts for Hispanic individuals. These are raw counts and do not adjust for racial population sizes, so differences may reflect city demographics rather than policing patterns. Instead, the variation suggests that each city’s combination of demographics, policing strategies, and enforcement contexts may shape the overall distribution of stops.\nTo explore this further and more meaningfully, I examined arrest rates by race within each city. While the previous plot shows the total number of stops, it does not account for differences in city populations or stop volumes. By calculating the percentage of stops that result in an arrest for each racial group, we can better compare enforcement outcomes across groups and cities. This approach provides a normalized measure that highlights potential disparities in arrests, rather than simply reflecting differences in the total number of stops.\n\n\nCode\n\n# Count total stops and arrests by race for Oakland and San Jose\n# Select race, outcome, and assign city labels\n# Combine both cities into one dataset\n\nSELECT \n    city,\n    subject_race AS race,\n    COUNT(*) AS total_stops,\n    SUM(CASE WHEN outcome = 'Arrest' THEN 1 ELSE 0 END) AS arrests\nFROM (\n    # Oakland data with city label\n    SELECT subject_race, outcome, 'Oakland' AS city \n    FROM ca_oakland_2020_04_01\n\n    UNION ALL\n\n    # San Jose data with city label\n    SELECT subject_race, outcome, 'San Jose' AS city \n    FROM ca_san_jose_2020_04_01\n) AS combined\n# Only include rows where race is not missing\nWHERE subject_race IS NOT NULL\n# Group results by city and race\nGROUP BY city, subject_race\n# Order results by city and race\nORDER BY city, race;\n\n\n\n\n\n\n\n\n\n\n\nThis bar chart shows the percentage of stops resulting in an arrest in Oakland and San Jose for each racial group. By comparing percentages rather than raw stop counts, we can meaningfully assess how likely a stop is to result in an arrest for different racial groups within each city. In Oakland, Black individuals have the highest arrest rate relative to other racial groups, suggesting that a larger share of stops involving Black individuals result in arrests. In San Jose, Black individuals also show the highest arrest rate, but it is closer to that of the other racial groups. Arrest rates vary by race and by city, indicating that enforcement outcomes differ across groups.\nUsing arrest rates instead of raw counts provides a normalized, comparable measure across cities and racial groups. While the plot highlights disparities in arrest outcomes, it does not prove bias or causation, as it does not account for underlying factors such as the nature of stops, location, or population demographics. This plot is a more meaningful exploration than raw stop counts because it allows readers to see potential disparities in outcomes rather than simply differences in the number of stops.\nThe datasets I used for this analysis were drawn from the following source:\nPierson, E., Simoiu, C., Overgoor, J., Corbett-Davies, S., Ramachandran, V., Phillips, C., & Goel, S. (2020). A large-scale analysis of racial disparities in police stops across the United States. Nature Human Behaviour, 4(7), 736–745. Stanford Open Policing Project (2020). Traffic stops database. https://openpolicing.stanford.edu/\n\n\nCode\nDBI::dbDisconnect(con_traffic, shutdown = TRUE)"
  },
  {
    "objectID": "whales and dolphins.html",
    "href": "whales and dolphins.html",
    "title": "Cetacean species birth rate over time",
    "section": "",
    "text": "This dataset compiles recorded births for Cetacean species and associated information including their birth year, sex, location, mother, father, status, etc. This plot shows the total births per year grouped by species to highlight long-term trends in population changes.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(viridis)  # for better contrasting colors\n\n# Load data\nallCetaceanData &lt;- read_csv(\"allCetaceanData.csv\")\n\n# Filter for years 2000–2020 and count births per species per year\ncetacean_summary &lt;- allCetaceanData |&gt;\n  filter(!is.na(birthYear), birthYear &gt;= 2000, birthYear &lt;= 2020) |&gt;\n  count(birthYear, species)\n\n# Plot stacked bar chart\nggplot(cetacean_summary, aes(x = birthYear, y = n, fill = species)) +\n  geom_col(position = \"stack\") +                     # stack bars by species\n  labs(\n    x = \"Year\",\n    y = \"Number of Births\",\n    title = \"Cetacean Births by Species (2000–2020)\",\n    fill = \"Species\"\n  ) +\n  scale_fill_viridis_d(option = \"turbo\") +          # distinct colors for species\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis stacked bar chart shows the number of cetacean births each year from 2000 to 2020, with each species represented by a different color. Some species have consistently higher birth counts, while others fluctuate more from year to year. For example, there are very many births per year in the Bottlenose species, and very few and inconsistent birth rates in the Atlantic Spotted species.The chart highlights species-level differences in reproduction and provides insight into population trends. These birth trends can inform conservation strategies and help researchers understand patterns in cetacean births over time.\nThe Cetacean dataset I worked with comes from the TidyTuesday database.\nThe original data set was made by Amber Thomas in her article for The Pudding.\n[link] TidyTuesday (2018-12-18)\nThomas, Amber, Cetacean Insights: What We’ve Learned From Whales and Dolphins in Captivity. The Pudding 2017. https://pudding.cool/2017/07/cetaceans/"
  },
  {
    "objectID": "seasons.html",
    "href": "seasons.html",
    "title": "American Idol Seasons Judges Appearances",
    "section": "",
    "text": "This dataset compiles the American idol winners and runner ups of the first 18 seasons, as well as release date, judges’ names, finals venue, number of episodes of the season, and original network and host. This plot compares judges’ appearance across all seasons which helps us understand the structure of the show over time.\n\n\nCode\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# Load seasons data\nseasons &lt;- read_csv(\"seasons.csv\")\n\n# Split judges into individual rows per season\njudge_links &lt;- seasons |&gt;\n  select(season, judges) |&gt;\n  separate_rows(judges, sep = \"; \") |&gt;\n  mutate(judges = trimws(judges))\n\n# Plot heatmap of judge appearances\nggplot(judge_links, aes(x = factor(season), y = judges)) +\n  geom_tile(fill = \"pink\") +\n  theme_minimal() +\n  labs(\n    title = \"Frequency of Judge Appearance per Season\",\n    x = \"Season\",\n    y = \"Judge\"\n  )\n\n\n\n\n\n\n\n\n\nThe heatmap shows which judges appeared in each season of American Idol. Each row represents a judge, and each column represents a season. Pink tiles indicate that the judge was present that season. From the plot, we can see that some judges, like Randy Jackson, appear across many seasons, while others, such as Mariah Carey, only appear briefly. This highlights both the continuity and turnover among judges throughout the show’s first 18 seasons, giving insight into how the panel changed over time and how that might have influenced the show’s style and judging dynamics.\nThe American Idol dataset I worked with comes from the TidyTuesday database, at the following link: https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-07-23/readme.md\nThe original dataset comes from Wikipedia.\nAmerican Idol. (2025). In Wikipedia. https://en.wikipedia.org/w/index.php?title=American_Idol&oldid=1326137286"
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Ethics: The Big Data era of Mosaicked Deidentification",
    "section": "",
    "text": "In 2006, Netflix, a large streaming service, released a dataset containing nearly 500,000 users’ movie ratings as part of a public machine-learning competition called the Netflix Prize. The goal of this competition was to improve the recommendation algorithms for users, by aiming to make them more personalized to improve user experience and satisfaction, and generate profit. The dataset released and used for this competition contained about 500,000 user ratings, associated with the user ID, some account information, the rated movie’s ID, and the timestamp of the rating. When Netflix released it for the competition, user IDs and account information was removed. The consent structure was not followed, but actually just assumed. Users didn’t consent to their data being released for this competition. Because that was done, Netflix claimed that user privacy and anonymity was guaranteed (Leetaru, 2016).\nThis created a privacy scandal, because it wasn’t the case. To explore how easy it would be to deanonymize users from this dataset, Arvind Narayanan and Vitaly Shmatikov combined it with publicly available IMDb rating histories in 2008. To do so, they used a method called mosaicking, which consists in combining data from one data source with another external data source (Narayanan & Shmatikov, 2008). They matched movie IDs rated in both datasets, and were able to trace back to a specific user that rated movies on both platforms, because each user has a unique behavioral fingerprint established by timestamp, movie rated and rating chosen on a scale of 1 to 5. therefore, the data was only partially anonymized. This method yielded high accuracy, and showed how easy it was to identify users and look at their entire viewing history, relating to personal information such as politics, sexuality, religion, personal interests and hobbies.\nEven though the data wasn’t initially intended to be used for secondary purposes such as cross-referencing in this case, this shows how risky and dangerous it is to release data containing traces of personal information.\nAnother issue that this case has enlightened is the numerous biases it comes with. For example, highly engaged users on movie rating platforms and Netflix subscribers, often the younger generation are disproportionately affected because the group isn’t random. On the other hand, Netflix benefited from this data collection and release that allowed the streaming platform to gain useful insights into how they can improve the recommendation algorithms. This results in a power imbalance between users and companies.\nThis story matters because it served as a case study to remind companies of the importance of establishing clear and working privacy policies. Since then, privacy has been regulated. For example, the general data protection regulation was adopted in the European Union in 2016, and many laws regulate minors’ privacy and ask for more extensive consent forms and opt-ins.\nReferences\nLeetaru, K. (2016). The Big Data Era of Mosaicked Deidentification: Can We Anonymize Data Anymore? Forbes.\nNarayanan, A., & Shmatikov, V. (2008). Robust de-anonymization of large sparse datasets. IEEE Symposium on Security and Privacy, 111–125."
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Netflix text analysis",
    "section": "",
    "text": "The dataset used in this analysis comes from the TidyTuesday database, which provides weekly structured datasets for data science practice. The original information is sourced from Flixable, a third-party Netflix search engine that tracks titles available on the platform. It was then made public by Shivam Bansal on a website called Kaggle. The dataset includes every movie and TV show listed on Netflix at the time of data collection, along with variables such as title, release year, country, genre, type, and brief descriptions.\nUsing this dataset, I conducted three text-based analyses to explore trends in Netflix’s content catalog over time. These include examining shifts in the prevalence of the words Love or War in titles, comparing the growth of movies versus TV shows, and analyzing how the distribution of top genres has changed across different time periods.\n\n\nCode\nlibrary(tidyverse)\nlibrary(readr)\n\nnetflix_titles &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-20/netflix_titles.csv')\n\nnetflix &lt;- netflix_titles\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(scales)\n\n# Subset titles containing Love or War as whole words\n# Detect the exact words \"Love\" or \"War\" using lookarounds:\n# (?&lt;!\\\\w) ensures the word is not preceded by a letter (start of word)\n# (?!\\\\w) ensures the word is not followed by a letter (end of word)\n# This prevents matching words like \"Loveful\" or \"Warfare\".\nnetflix_love_war &lt;- netflix |&gt;\n  filter(str_detect(title, \"(?&lt;!\\\\w)(Love|War)(?!\\\\w)\")) |&gt;\n  select(title, type, country, release_year)\n\n# Count total titles per year\ntotal_by_year &lt;- netflix |&gt;\n  count(release_year, name = \"total_titles\")\n\n# Count Love/War titles per year\nlove_war_by_year &lt;- netflix_love_war |&gt;\n  count(release_year, name = \"love_war_titles\")\n\n# Merge + compute proportions\nlove_war_prop &lt;- left_join(love_war_by_year, total_by_year, by = \"release_year\") |&gt;\n  mutate(prop_titles = love_war_titles / total_titles)\n\n# Plot\nggplot(love_war_prop, aes(x = release_year, y = prop_titles)) +\n  geom_col(fill = \"pink\") +\n  geom_line(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Proportion of Netflix Titles Containing the words 'Love' or 'War' Over Time\",\n    x = \"Release Year\",\n    y = \"Proportion of Titles\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis analysis examined titles containing the thematic keywords “Love” or “War.” The resulting visualization shows an early peak around 1940, followed by no data for many years, and then a much lower peak around 1975 and finally a stable proportion beginning in the 2000s. This pattern is shaped both by the start of available data and by cultural trends in how films and shows were titled. The prominence of “Love” and “War” in the early years likely reflects historical influences—including the popularity of romance-focused or war-influenced media around that time—while modern Netflix titles tend to avoid explicit thematic words in favor of more abstract or stylistic naming.\n\n\nCode\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(scales)\n\n# Load Netflix data\nnetflix_titles &lt;- read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-20/netflix_titles.csv\"\n)\n\n# Select only variables needed\nnetflix &lt;- netflix_titles\n\n# Create a dataset that counts how many titles of each type appear per year\nnetflix_proportions &lt;- netflix |&gt;\n  filter(!is.na(release_year)) |&gt;   # remove missing years\n  group_by(release_year, type) |&gt;   # count Titles by year AND type (Movie/TV Show)\n  summarise(n_titles = n(), .groups = \"drop\") |&gt;\n  group_by(release_year) |&gt;         # within each year...\n  mutate(prop_titles = n_titles / sum(n_titles)) |&gt;  # ...compute proportion per type\n  ungroup()\n\n# Plot proportions of Movies vs TV Shows over time\nggplot(netflix_proportions, aes(x = release_year, y = prop_titles, fill = type)) +\n  geom_col(position = \"stack\") +    # stacked bars show total 100%\n  scale_fill_manual(values = c(\"Movie\" = \"steelblue\", \"TV Show\" = \"orange\")) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +  # y-axis as percentages\n  labs(\n    title = \"Proportion of Movies vs TV Shows on Netflix Over Time\",\n    subtitle = \"Stacked proportions show how the platform's focus shifts across years\",\n    x = \"Release Year\",\n    y = \"Proportion of Titles\",\n    fill = \"Content Type\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot analysis compares the proportion of Movies vs. TV Shows released each year. It shows a clear long-term shift in Netflix’s catalog: early years are dominated by movies, but over time, TV shows grow to represent a much larger share. This aligns with Netflix’s business transition, especially after the mid-2010s, when the platform increased investment in streaming services as audience viewing increased.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(scales)\nlibrary(readr)\n\n# Extract main genre from 'listed_in' and create uppercase version\n# Also flag if genre is 'Action'\nnetflix_genre &lt;- netflix_titles |&gt;\n  mutate(\n    main_genre = str_extract(listed_in, \"^[^,]+\"),  # take first genre before comma\n    main_genre_upper = str_to_upper(main_genre),\n    is_action = str_detect(main_genre, \"Action\")     # flag Action titles\n  ) |&gt;\n  drop_na(release_year, main_genre_upper) |&gt;        # remove missing release_year or genre\n  filter(release_year &gt;= 1950)                     # filter to reasonable years\n\n# Identify the top 6 genres by count\ntop_genres &lt;- netflix_genre |&gt;\n  count(main_genre_upper, sort = TRUE) |&gt;\n  slice_head(n = 6) |&gt;\n  pull(main_genre_upper)\n\n# Filter data to top genres and create periods\nnetflix_top_genres &lt;- netflix_genre |&gt;\n  filter(main_genre_upper %in% top_genres) |&gt;\n  mutate(\n    period = case_when(\n      release_year &lt; 2010 ~ \"Before 2010\",\n      release_year &gt;= 2010 & release_year &lt; 2020 ~ \"2010–2019\",\n      release_year &gt;= 2020 ~ \"2020–Present\"\n    ),\n    period = factor(period, levels = c(\"Before 2010\", \"2010–2019\", \"2020–Present\"))  # ensure order\n  )\n\n# Calculate proportion of each genre per period\ngenre_trends &lt;- netflix_top_genres |&gt;\n  count(period, main_genre_upper, is_action) |&gt;\n  group_by(period) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  ungroup()\n\n# Plot the evolution of top genres over time\nggplot(genre_trends, aes(x = period, y = prop, fill = main_genre_upper)) +\n  geom_col(position = position_dodge(width = 0.8)) +      # dodge bars for clarity\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  labs(\n    title = \"Evolution of Top Netflix Genres Over Time\",\n    subtitle = \"Each genre shown separately, one color per genre\",\n    x = \"Release Period\",\n    y = \"Proportion of Titles\",\n    fill = \"Genre\"\n  ) +\n  theme_minimal(base_size = 9) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nThis analysis focuses on genre trends, isolating the top six most common genres and comparing their relative proportions across three major time periods. By grouping titles into “Before 2010,” “2010–2019,” and “2020–Present,” the plot highlights how the platform’s genre landscape has diversified over time. Some genres, such as Action, remain consistently represented, while others rise or fall depending on industry trends, Netflix’s expansion into international markets, and shifts in audience demand. These changes help illustrate how Netflix has broadened its catalog and adapted its offerings to an increasingly global subscriber base.\nThese three analyses show that Netflix’s library has transformed significantly over the past several decades. Title conventions have become less tied to explicit themes like love and war; the platform has pivoted heavily toward TV shows; and the genre distribution has become more varied and dynamic. These trends reflect both changing cultural patterns in media production and Netflix’s evolution from a DVD rental service to one of the largest streaming services.\nShivam Bansal scraped this data and made it publicly available on Kaggle at the following link:\nhttps://www.kaggle.com/datasets/shivamb/netflix-shows\nThe dataset I used lives in the Tidy Tuesday database at this link: https://github.com/rfordatascience/tidytuesday/blob/main/data/2021/2021-04-20/readme.md"
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Diabetes simulation study",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nglobal_mortality &lt;- read_csv(\"global_mortality.csv\")\nview(global_mortality)\n\n\nThis dataset presents the mortality rate for different diseases, per year (from 1990 to 2016) and per country. We are exploring the specific average rate of mortality from diabetes in the United States and in France across all years. The dataset I analyzed comes from the Tidy Tuesday database and is available at the following link: https://github.com/rfordatascience/tidytuesday/tree/main/data/2018/2018-04-16\nThe original data comes from an article entitled, “Causes of death” and published in the Our World in Data magazine by Saloni Dattani, Fiona Spooner, Hannah Ritchie and Max Roser at the following link: Dattani, S., Spooner, F., Ritchie, H., & Roser, M. (2023). Causes of Death. Our World in Data. https://ourworldindata.org/causes-of-death\nIn this simulation study, we will explore whether France and the United States have the same average diabetes mortality rate. We will first calculate the observed difference in mean diabetes mortality between the two countries across all available years (1990-2016). To test whether the difference we observe could have been a result of random distribution, we will run a permutation test shuffling the diabetes mortality percentage rate per country 1000 times, to create a null distribution. By treating the observed years as a sample, we can use a permutation test to assess whether the observed difference in average mortality is larger than we would expect from random variation in yearly mortality. This allows us to generalize conclusions beyond the years in the dataset.\nThe research question we are answering is the following: Do France and the United States differ in their underlying diabetes mortality processes, such that the observed differences in yearly diabetes mortality are larger than what we would expect due to random chance?\nThe null hypothesis (Ho): The underlying process generating the diabetes mortality rate is the same in France and the US. Any observed difference in yearly mortality rates is due to random variation across years.\nThe alternative hypothesis (Ha): The underlying process differs between the two countries, leading to different average diabetes mortality rates.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\n# Filter data for France and US, select relevant columns\nmortality_filtered &lt;- global_mortality |&gt;\n  filter(country %in% c(\"France\", \"United States\")) |&gt;\n  select(country, year, `Diabetes (%)`) |&gt;\n  rename(diabetes_pct = `Diabetes (%)`)  # rename column for easier handling\n\n\n\n\nCode\n# Compute average diabetes mortality per country\nmortality_avg &lt;- mortality_filtered |&gt;\n  group_by(country) |&gt;\n  summarize(avg_diabetes = mean(diabetes_pct))\n\n# Plot the averages for visual comparison\nggplot(mortality_avg, aes(x = country, y = avg_diabetes, fill = country)) +\n  geom_col(show.legend = FALSE) +\n  labs(\n    title = \"Average Diabetes Mortality (%) Across All Years\",\n    x = \"Country\",\n    y = \"Average Diabetes Mortality (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe main variable of interest is the percentage of diabetes mortality rate. While the dataset doesn’t include a clear statement of what this variable represents, we can infer based on the name of the variable and the dataset itself, that it represents the percentage of deaths in a given year that were attributed to diabetes among all deaths.\nWe observe a difference in average mortality rate between the two countries, so now we are going to test if this is a result of the random distribution model, by establishing a null distribution. Instead of comparing only the two countries’ averages, we will repeatedly shuffle the yearly diabetes mortality percentages between the two countries, recompute the average mortality for each country, and record the difference. By repeating this process 1000 times, we can see how large the difference would be between the two countries if we were following the random chance model.\n\n\n\n\n\n\n\n\n\nThis permutation test assesses whether the observed difference in average diabetes mortality between France and the United States is larger than would be expected if the underlying mortality process were the same in both countries. By shuffling the yearly mortality percentages 1,000 times, we generated a null distribution representing the differences we might see due to random yearly fluctuations. The observed difference falls far outside this null distribution (p ≈ 0), providing strong evidence that the underlying process generating diabetes mortality differs between France and the US, not just in the years 1990–2016 but as a general pattern. We can therefore reject the null hypothesis."
  },
  {
    "objectID": "my_slides.html#background",
    "href": "my_slides.html#background",
    "title": "Ethics on Netflix",
    "section": "Background",
    "text": "Background\n\nNetflix launched a public competition in 2006, goal is to predict how a user would rate a movie they hadn’t seen (split by team, $1million in prize money if you can improve system by 10%).\nGoal: improve movie recommendation algorithms that work using predictions based on what a user has previously watched\nPart of Netflix’s broader push to personalize user experience, increasing use of the platform, and user satisfaction"
  },
  {
    "objectID": "my_slides.html#methods",
    "href": "my_slides.html#methods",
    "title": "Ethics on Netflix",
    "section": "Methods",
    "text": "Methods\n\n500,000 users\nAbout 17,770 movies rated on a scale of 1-5 (1 being “dislike”, 5 being “love) with associated timestamp of rating, movie ID, user ID and account information\n=&gt; user ID and account information were removed for release for this competition"
  },
  {
    "objectID": "my_slides.html#terms-of-service-and-consent",
    "href": "my_slides.html#terms-of-service-and-consent",
    "title": "Ethics on Netflix",
    "section": "Terms of service and consent",
    "text": "Terms of service and consent\n\nNetflix never notified users about the data release for this competition\nNo explicit permission\nAssumed anonymization = consent (assuming it is safe because names were removed)\nTerms of service at the time allowed data use for “improving services,” ensured it wasn’t for public release"
  },
  {
    "objectID": "my_slides.html#deanonymization",
    "href": "my_slides.html#deanonymization",
    "title": "Ethics on Netflix",
    "section": "Deanonymization",
    "text": "Deanonymization\n\nNarayanan & Shmatikov (computer scientists, with expertise in data privacy) re-identified users in 2008 by matching Netflix ratings with public IMDb ratings\nOnly needed a few movies + approximate timestamps to identify people (each set of movies watched by an individual is unique)\nHigh accuracy: Netflix records could be linked to specific individuals\nRevealed personal preferences (politics, sexuality, religion, etc.) through viewing history"
  },
  {
    "objectID": "my_slides.html#hidden-identifiers",
    "href": "my_slides.html#hidden-identifiers",
    "title": "Ethics on Netflix",
    "section": "Hidden identifiers",
    "text": "Hidden identifiers\n\nNames removed, but unique behavioral fingerprints remained\nTimestamps, movies rated and rating styles (what a “good” movie’s rating is) are unique\nMosaicking: combining datasets from different sources to re-identify users"
  },
  {
    "objectID": "my_slides.html#bias",
    "href": "my_slides.html#bias",
    "title": "Ethics on Netflix",
    "section": "Bias",
    "text": "Bias\n\nDataset is biased toward:\n\nheavy users\nhighly engaged users\nspecific demographics (more men, younger population, highly educated groups)"
  },
  {
    "objectID": "my_slides.html#why-does-this-matter",
    "href": "my_slides.html#why-does-this-matter",
    "title": "Ethics on Netflix",
    "section": "Why does this matter?",
    "text": "Why does this matter?\n\nShows how “anonymous” data can still identify people\nProves removing names ≠ privacy\nDemonstrates potential risks of secondary use (use for other purposes)\nExposes power imbalance: companies control data, users don’t\nIt is a good case study for privacy risks =&gt; companies implement stricter consent policies"
  },
  {
    "objectID": "my_slides.html#why-was-i-interested-in-this",
    "href": "my_slides.html#why-was-i-interested-in-this",
    "title": "Ethics on Netflix",
    "section": "Why was I interested in this?",
    "text": "Why was I interested in this?\n\nExpands on stakes of another class I’m taking covering themes of ethics, surveillance and privacy\nNetflix User"
  }
]