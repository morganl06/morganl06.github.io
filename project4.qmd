---
title: "Ethics: The Big Data era of Mosaicked Deidentification"
---

### 

In 2006, Netflix, a large streaming service, released a dataset containing nearly 500,000 users’ movie ratings as part of a public machine-learning competition called the Netflix Prize. Netflix claimed that all personal information had been removed to protect user privacy (Leetaru, 2016). The purpose of releasing the dataset was to allow researchers to develop improved recommendation algorithms—that is, models that predict what movies a user might enjoy based on their past ratings. Although names and account IDs were removed, the dataset preserved detailed behavioral traces such as timestamps and rating patterns, which can be uniquely identifying. Additionally, multiple datasets can be combined to fill in one another’s missing pieces, a process known as “mosaicking,” which illustrates why seemingly anonymized data can still reveal identities (Leetaru, 2016).

In 2008, researchers Arvind Narayanan and Vitaly Shmatikov demonstrated that the Netflix Prize dataset was vulnerable to deanonymization by matching users’ Netflix rating patterns with publicly available rating histories from IMDb (Narayanan & Shmatikov, 2008). Even if someone rated only a handful of movies on IMDb, the combination of rating values and timestamps was often specific enough to identify their full Netflix record with high accuracy. This showed that the dataset—though intended for research—posed broader privacy risks: individuals’ preferences related to politics, sexuality, religion, or personal interests could be exposed without their knowledge or consent.

**Was the consent structure and permission followed?**

Although Netflix removed explicit identifiers, users were never informed that their ratings would be shared publicly for a research competition. No explicit permission was requested, and the release relied on the assumption that anonymization alone made the data safe (Leetaru, 2016). The users did **not** consent to this secondary use of their data. Moreover, because the dataset could be deanonymized, Netflix’s anonymization was insufficient to provide meaningful privacy protection. Individuals had no reasonable expectation that their private rating histories could be reconstructed and linked back to them using external data sources. The ethical issue is that Netflix treated anonymization as a substitute for consent, despite expert evidence that behavioral data is highly identifiable (Narayanan & Shmatikov, 2008).

**Was the data being used in unintended ways?**

Yes. Netflix released the dataset specifically to improve recommendation algorithms, but the data was not intended to be cross-referenced with external platforms to re-identify users. Narayanan and Shmatikov (2008) showed that such cross-linking was not only possible but highly accurate. This is an example of secondary use, where data collected for one purpose (recommender systems) is repurposed for another (deanonymization research), highlighting a key risk in data science. Users’ rating histories—originally meant only for their own viewing recommendations—became a potential vehicle for revealing sensitive personal information.

**Was the data actually anonymized?**

The data was only partially anonymized. Netflix removed names and replaced account IDs, but retained users’ complete behavioral profiles, which function like digital fingerprints. Rating sequences, timing patterns, and movie choices are distinctive enough to identify individuals with surprising accuracy. Narayanan and Shmatikov (2008) demonstrated that knowing a user’s approximate ratings on just a few films on IMDb could be sufficient to re-identify that user in the Netflix dataset. Once linked, the person’s full Netflix viewing history (content related to sexuality, mental health, religion, or politics) became exposed. This case illustrates that anonymization is not equivalent to privacy.

**Who was harmed, and who benefitted?**

Netflix benefited from crowdsourcing improvements to its recommendation algorithms at no financial cost, and academic researchers gained publications and benchmark data from the competition. However, the individuals harmed were ordinary Netflix users whose rating histories were shared without explicit permission. Some users who were deanonymized had sensitive movie preferences that could reveal personal identity attributes. Even without malicious intent, releasing re-identifiable behavioral data created real privacy risks once anonymization proved ineffective.

**Who was measured, and can results be generalized?**

The dataset represents only users who actively rate movies on Netflix, a non-random and self-selected subgroup. These users may differ from the general Netflix population in age, engagement, genre preferences, and rating behavior. Therefore, models trained on the Netflix Prize dataset generalize poorly to users who rarely rate movies or who consume content without providing feedback. This introduces bias and limits the external validity of any models or conclusions derived from the dataset.

**Why does this example matter for data science?**

The Netflix Prize case demonstrates how easy it is to re-identify individuals from behavioral data that companies claim is anonymous. It challenges the assumption that removing names is sufficient for privacy protection and highlights the fact that human behavior itself is highly identifiable. It also exposes a power imbalance: companies control the data and may share it widely for their own benefit, while users are often unaware their information is being repurposed in ways that could expose them to harm. The incident remains a foundational example in data ethics, showing the need for stronger protections, transparent consent processes, and caution when releasing real-world datasets.

**References**

Leetaru, K. (2016). *The Big Data Era of Mosaicked Deidentification: Can We Anonymize Data Anymore?* Forbes.

Narayanan, A., & Shmatikov, V. (2008). *Robust de-anonymization of large sparse datasets.* IEEE Symposium on Security and Privacy, 111–125.
