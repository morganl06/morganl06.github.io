---
title: "Ethics: The Big Data era of Mosaicked Deidentification"
---

### 

In 2006, Netflix, a large streaming service, released a dataset containing nearly 500,000 users’ movie ratings as part of a public machine-learning competition called the Netflix Prize. The goal of this competition was to improve the recommendation algorithms for users, by aiming to make them more personalized to improve user experience and satisfaction, and generate profit. The dataset released and used for this competition contained about 500,000 user ratings, associated with the user ID, some account information, the rated movie's ID, and the timestamp of the rating. When Netflix released it for the competition, user IDs and account information was removed. The consent structure was not followed, but actually just assumed. Users didn't consent to their data being released for this competition. Because that was done, Netflix claimed that user privacy and anonymity was guaranteed (Leetaru, 2016).

This created a privacy scandal, because it wasn't the case. To explore how easy it would be to deanonymize users from this dataset, Arvind Narayanan and Vitaly Shmatikov combined it with publicly available IMDb rating histories in 2008. To do so, they used a method called mosaicking, which consists in combining data from one data source with another external data source (Narayanan & Shmatikov, 2008). They matched movie IDs rated in both datasets, and were able to trace back to a specific user that rated movies on both platforms, because each user has a unique behavioral fingerprint established by timestamp, movie rated and rating chosen on a scale of 1 to 5. therefore, the data was only partially anonymized. This method yielded high accuracy, and showed how easy it was to identify users and look at their entire viewing history, relating to personal information such as politics, sexuality, religion, personal interests and hobbies.

Even though the data wasn't initially intended to be used for secondary purposes such as cross-referencing in this case, this shows how risky and dangerous it is to release data containing traces of personal information.

Another issue that this case has enlightened is the numerous biases it comes with. For example, highly engaged users on movie rating platforms and Netflix subscribers, often the younger generation are disproportionately affected because the group isn't random. On the other hand, Netflix benefited from this data collection and release that allowed the streaming platform to gain useful insights into how they can improve the recommendation algorithms. This results in a power imbalance between users and companies.

This story matters because it served as a case study to remind companies of the importance of establishing clear and working privacy policies. Since then, privacy has been regulated. For example, the general data protection regulation was adopted in the European Union in 2016, and many laws regulate minors' privacy and ask for more extensive consent forms and opt-ins.

**References**

Leetaru, K. (2016). *The Big Data Era of Mosaicked Deidentification: Can We Anonymize Data Anymore?* Forbes.

Narayanan, A., & Shmatikov, V. (2008). *Robust de-anonymization of large sparse datasets.* IEEE Symposium on Security and Privacy, 111–125.
